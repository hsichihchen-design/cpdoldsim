"""
DataManager - è³‡æ–™ç®¡ç†æ¨¡çµ„ (ä¿®æ”¹ç‰ˆï¼šæ”¯æ´é€²è²¨è³‡æ–™å’ŒåŠ ç­é‚è¼¯)
è² è²¬è¼‰å…¥ã€é©—è­‰ã€é è™•ç†æ‰€æœ‰master dataå’Œtransaction data
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, Optional, Tuple
import sys
from datetime import datetime, date

# ä¿®æ­£ï¼šä½¿ç”¨å‹•æ…‹è·¯å¾‘è¨­å®š
try:
    from config import MASTER_DATA_FILES, TRANSACTION_DATA_FILES
except ImportError:
    # å¦‚æœconfig.pyä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­è·¯å¾‘
    MASTER_DATA_FILES = {
        'system_parameters': Path('data/master_data/system_parameters.csv'),
        'item_master': Path('data/master_data/item_master.csv'),
        'staff_skill_master': Path('data/master_data/staff_skill_master.csv'),
        'workstation_capacity': Path('data/master_data/workstation_capacity.csv'),
        'route_schedule_master': Path('data/master_data/route_schedule_master.csv'),
        'item_inventory': Path('data/master_data/item_inventory.csv'),
        'branch_route_master': Path('data/master_data/branch_route_master.csv')
    }
    
    TRANSACTION_DATA_FILES = {
        'historical_orders': Path('data/transaction_data/historical_orders.csv'),
        'historical_receiving': Path('data/transaction_data/historical_receiving.csv')  # ğŸ†• ç¢ºä¿åŒ…å«é€²è²¨è³‡æ–™
    }

class DataManager:
    def __init__(self):
        """åˆå§‹åŒ–DataManager"""
        self.logger = logging.getLogger(__name__)
        self.master_data = {}
        self.transaction_data = {}
        self.validation_results = {}
        
        # ğŸ†• æ–°å¢ï¼šé€²è²¨ç›¸é—œè³‡æ–™è¿½è¹¤
        self.receiving_data_available = False
        self.receiving_date_range = None
        
    def load_master_data(self) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥æ‰€æœ‰master dataæª”æ¡ˆ"""
        self.logger.info("é–‹å§‹è¼‰å…¥Master Data...")
        
        for data_name, file_path in MASTER_DATA_FILES.items():
            try:
                if file_path.exists():
                    # å˜—è©¦ä¸åŒç·¨ç¢¼
                    encodings_to_try = ['utf-8', 'cp1252', 'gbk', 'big5']
                    df = None
                    
                    for encoding in encodings_to_try:
                        try:
                            df = pd.read_csv(file_path, encoding=encoding)
                            self.logger.info(f"âœ… è¼‰å…¥ {data_name} (ç·¨ç¢¼: {encoding}): {len(df)} ç­†è³‡æ–™")
                            break
                        except UnicodeDecodeError:
                            continue
                    
                    if df is not None:
                        # ğŸ†• æ–°å¢ï¼šæ•¸æ“šæ¸…ç†
                        df = self._clean_master_data(df, data_name)
                        
                        # ç‰¹æ®Šè™•ç†ï¼šç³»çµ±åƒæ•¸è³‡æ–™
                        if data_name == 'system_parameters':
                            df = self._validate_system_parameters(df)
                        
                        self.master_data[data_name] = df
                    else:
                        self.logger.error(f"âŒ ç„¡æ³•ä»¥ä»»ä½•ç·¨ç¢¼è¼‰å…¥ {data_name}")
                        
                else:
                    self.logger.warning(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
                    
            except Exception as e:
                self.logger.error(f"âŒ è¼‰å…¥ {data_name} å¤±æ•—: {str(e)}")
                
        return self.master_data
        
    def load_transaction_data(self, start_date: str = None, end_date: str = None, 
                            filter_valid_items: bool = True) -> Dict[str, pd.DataFrame]:
        """ğŸ”§ ä¿®æ”¹ï¼šè¼‰å…¥äº¤æ˜“è³‡æ–™ï¼ˆå¼·åŒ–é€²è²¨è³‡æ–™è™•ç†ï¼‰"""
        self.logger.info(f"é–‹å§‹è¼‰å…¥Transaction Data (æ—¥æœŸç¯„åœ: {start_date} - {end_date})...")
        
        for data_name, file_path in TRANSACTION_DATA_FILES.items():
            try:
                if file_path.exists():
                    df = pd.read_csv(file_path, encoding='utf-8')
                    
                    # ğŸ†• ç‰¹æ®Šè™•ç†ï¼šé€²è²¨è³‡æ–™
                    if data_name == 'historical_receiving':
                        df = self._process_receiving_data(df, start_date, end_date)
                        self.receiving_data_available = len(df) > 0
                        
                        if self.receiving_data_available:
                            self._analyze_receiving_data_range(df)
                    
                    # æ—¥æœŸç¯©é¸é‚è¼¯ï¼ˆå¦‚æœæä¾›ï¼‰
                    if start_date or end_date:
                        df = self._filter_by_date(df, start_date, end_date, data_name)
                    
                    # é›¶ä»¶éæ¿¾
                    if filter_valid_items:
                        df = self.filter_valid_items(df)
                    
                    self.transaction_data[data_name] = df
                    self.logger.info(f"âœ… è¼‰å…¥ {data_name}: {len(df)} ç­†è³‡æ–™")
                else:
                    self.logger.warning(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
                    
            except Exception as e:
                self.logger.error(f"âŒ è¼‰å…¥ {data_name} å¤±æ•—: {str(e)}")
                
        return self.transaction_data
    
    def _validate_system_parameters(self, params_df: pd.DataFrame) -> pd.DataFrame:
        """ğŸ†• é©—è­‰ç³»çµ±åƒæ•¸å®Œæ•´æ€§"""
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ['parameter_name', 'parameter_value', 'data_type']
        missing_columns = [col for col in required_columns if col not in params_df.columns]
        
        if missing_columns:
            self.logger.error(f"ç³»çµ±åƒæ•¸æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
            return params_df
        
        # æª¢æŸ¥æ–°å¢çš„å¿…è¦åƒæ•¸
        required_new_params = [
            'receiving_completion_days',
            'shift_start_time', 
            'shift_end_time',
            'overtime_enabled'
        ]
        
        existing_params = params_df['parameter_name'].tolist()
        missing_params = [param for param in required_new_params if param not in existing_params]
        
        if missing_params:
            self.logger.warning(f"ç³»çµ±åƒæ•¸æª”æ¡ˆç¼ºå°‘æ–°å¢åƒæ•¸: {missing_params}")
            self.logger.info("è«‹åŸ·è¡Œ system_parameters_update.py æ›´æ–°åƒæ•¸æª”æ¡ˆ")
        
        # é©—è­‰é‡è¦åƒæ•¸çš„è³‡æ–™å‹æ…‹
        type_validations = {
            'receiving_completion_days': 'integer',
            'max_overtime_hours': 'float',
            'overtime_enabled': 'string'
        }
        
        for param_name, expected_type in type_validations.items():
            param_row = params_df[params_df['parameter_name'] == param_name]
            if len(param_row) > 0:
                actual_type = param_row.iloc[0]['data_type']
                if actual_type != expected_type:
                    self.logger.warning(f"åƒæ•¸ {param_name} è³‡æ–™å‹æ…‹ä¸ç¬¦: æœŸæœ› {expected_type}, å¯¦éš› {actual_type}")
        
        return params_df
    
    def _process_receiving_data(self, receiving_df: pd.DataFrame, start_date: str = None,end_date: str = None) -> pd.DataFrame:
        # è™•ç†ä¸åŒçš„æ•¸é‡æ¬„ä½åç¨±
        if 'INVQTY' in receiving_df.columns and 'QTY' not in receiving_df.columns:
            receiving_df['QTY'] = receiving_df['INVQTY']
        
        """ğŸ†• è™•ç†é€²è²¨è³‡æ–™"""
        self.logger.info(f"è™•ç†é€²è²¨è³‡æ–™: {len(receiving_df)} ç­†åŸå§‹è³‡æ–™")
        
        if len(receiving_df) == 0:
            return receiving_df
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = ['DATE', 'FRCD', 'PARTNO', 'QTY']
        missing_columns = [col for col in required_columns if col not in receiving_df.columns]
        
        # ğŸ”§ æ–°å¢ï¼šè™•ç†ä¸åŒçš„æ•¸é‡æ¬„ä½åç¨±
        if 'INVQTY' in receiving_df.columns and 'QTY' not in receiving_df.columns:
            receiving_df['QTY'] = receiving_df['INVQTY']
            self.logger.info("å°‡ INVQTY æ¬„ä½é‡æ–°å‘½åç‚º QTY")

        # é‡æ–°æª¢æŸ¥å¿…è¦æ¬„ä½
        missing_columns = [col for col in required_columns if col not in receiving_df.columns]

        if missing_columns:
            self.logger.error(f"é€²è²¨è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # è™•ç†DATEæ¬„ä½
        try:
            # å˜—è©¦è½‰æ›æ—¥æœŸæ ¼å¼
            if receiving_df['DATE'].dtype == 'object':
                # è™•ç†å­—ä¸²æ ¼å¼çš„æ—¥æœŸ
                receiving_df['DATE'] = pd.to_datetime(receiving_df['DATE'], errors='coerce')
            
            # ç§»é™¤ç„¡æ•ˆæ—¥æœŸçš„è¨˜éŒ„
            invalid_dates = receiving_df['DATE'].isna()
            if invalid_dates.sum() > 0:
                self.logger.warning(f"ç§»é™¤ {invalid_dates.sum()} ç­†ç„¡æ•ˆæ—¥æœŸçš„é€²è²¨è¨˜éŒ„")
                receiving_df = receiving_df[~invalid_dates]
            
        except Exception as e:
            self.logger.error(f"è™•ç†é€²è²¨æ—¥æœŸæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return pd.DataFrame()
        
        # è™•ç†QTYæ¬„ä½
        try:
            receiving_df['QTY'] = pd.to_numeric(receiving_df['QTY'], errors='coerce')
            receiving_df['QTY'] = receiving_df['QTY'].fillna(0).astype(int)
            
            # ç§»é™¤æ•¸é‡ç‚º0æˆ–è² æ•¸çš„è¨˜éŒ„
            invalid_qty = receiving_df['QTY'] <= 0
            if invalid_qty.sum() > 0:
                self.logger.warning(f"ç§»é™¤ {invalid_qty.sum()} ç­†ç„¡æ•ˆæ•¸é‡çš„é€²è²¨è¨˜éŒ„")
                receiving_df = receiving_df[~invalid_qty]
                
        except Exception as e:
            self.logger.error(f"è™•ç†é€²è²¨æ•¸é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        
        # ğŸ†• æ–°å¢ï¼šå¦‚æœæ²’æœ‰RECEIVING_IDï¼Œè‡ªå‹•ç”Ÿæˆ
        if 'RECEIVING_ID' not in receiving_df.columns:
            receiving_df['RECEIVING_ID'] = range(1, len(receiving_df) + 1)
            receiving_df['RECEIVING_ID'] = 'RCV_' + receiving_df['RECEIVING_ID'].astype(str).str.zfill(6)
        
        self.logger.info(f"âœ… é€²è²¨è³‡æ–™è™•ç†å®Œæˆ: {len(receiving_df)} ç­†æœ‰æ•ˆè³‡æ–™")
        
        return receiving_df
    
    def _analyze_receiving_data_range(self, receiving_df: pd.DataFrame):
        """ğŸ†• åˆ†æé€²è²¨è³‡æ–™çš„æ—¥æœŸç¯„åœ"""
        if len(receiving_df) == 0 or 'DATE' not in receiving_df.columns:
            return
        
        try:
            min_date = receiving_df['DATE'].min()
            max_date = receiving_df['DATE'].max()
            
            self.receiving_date_range = {
                'start_date': min_date,
                'end_date': max_date,
                'total_days': (max_date - min_date).days + 1,
                'record_count': len(receiving_df)
            }
            
            self.logger.info(f"ğŸ“Š é€²è²¨è³‡æ–™ç¯„åœ: {min_date.strftime('%Y-%m-%d')} åˆ° {max_date.strftime('%Y-%m-%d')} ({self.receiving_date_range['total_days']} å¤©)")
            
            # æŒ‰æ—¥æœŸçµ±è¨ˆé€²è²¨ç­†æ•¸
            daily_counts = receiving_df.groupby(receiving_df['DATE'].dt.date).size()
            avg_daily = daily_counts.mean()
            
            self.logger.info(f"ğŸ“Š å¹³å‡æ¯æ—¥é€²è²¨: {avg_daily:.1f} ç­†")
            
            # è­˜åˆ¥é€²è²¨é«˜å³°æ—¥
            peak_days = daily_counts[daily_counts > avg_daily * 1.5].head(5)
            if len(peak_days) > 0:
                self.logger.info(f"ğŸ“Š é€²è²¨é«˜å³°æ—¥: {list(peak_days.index)}")
                
        except Exception as e:
            self.logger.error(f"åˆ†æé€²è²¨è³‡æ–™ç¯„åœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    def _filter_by_date(self, df: pd.DataFrame, start_date: str, end_date: str, data_name: str, workdays_only: bool = True) -> pd.DataFrame:
        """ğŸ”§ ä¿®æ”¹ï¼šæ ¹æ“šæ—¥æœŸç¯©é¸è³‡æ–™ï¼ˆæ”¯æ´ä¸åŒè³‡æ–™é¡å‹ï¼‰"""
        
        # ç¢ºå®šæ—¥æœŸæ¬„ä½åç¨±
        date_column = None
        possible_date_columns = ['DATE', 'date', 'Date', 'ORDERDATE', 'ORDER_DATE']
        
        for col in possible_date_columns:
            if col in df.columns:
                date_column = col
                break
        
        if not date_column:
            self.logger.warning(f"{data_name} æ‰¾ä¸åˆ°æ—¥æœŸæ¬„ä½ï¼Œè·³éæ—¥æœŸç¯©é¸")
            return df
        
        try:
            # ç¢ºä¿æ—¥æœŸæ¬„ä½æ˜¯datetimeæ ¼å¼
            if df[date_column].dtype != 'datetime64[ns]':
                df[date_column] = pd.to_datetime(df[date_column])
            
            original_count = len(df)
            
            if start_date:
                start_dt = pd.to_datetime(start_date)
                df = df[df[date_column] >= start_dt]
                
            if end_date:
                end_dt = pd.to_datetime(end_date)
                df = df[df[date_column] <= end_dt]
            
            filtered_count = len(df)
            
            if filtered_count < original_count:
                self.logger.info(f"ğŸ—“ï¸ {data_name} æ—¥æœŸç¯©é¸: {original_count} â†’ {filtered_count} ç­†")
            
        except Exception as e:
            self.logger.error(f"æ—¥æœŸç¯©é¸å¤±æ•— ({data_name}): {str(e)}")

        # ğŸ†• æ–°å¢ï¼šå·¥ä½œæ—¥ç¯©é¸
        if workdays_only and date_column:
            pre_workday_count = len(df)
            workday_mask = df[date_column].apply(lambda x: self.is_workday(x))
            df = df[workday_mask]
            post_workday_count = len(df)
            
            if post_workday_count < pre_workday_count:
                weekend_filtered = pre_workday_count - post_workday_count
                self.logger.info(f"ğŸ“… {data_name} å·¥ä½œæ—¥ç¯©é¸: ç§»é™¤ {weekend_filtered} ç­†é€±æœ«è³‡æ–™")
        
        return df
    
    def validate_data_consistency(self) -> Dict[str, bool]:
        """ğŸ”§ ä¿®æ”¹ï¼šæª¢æŸ¥è³‡æ–™ä¸€è‡´æ€§ï¼ˆæ–°å¢é€²è²¨è³‡æ–™æª¢æŸ¥ï¼‰"""
        self.logger.info("é–‹å§‹æª¢æŸ¥è³‡æ–™ä¸€è‡´æ€§...")
        
        validation_results = {}
        
        # æª¢æŸ¥Master Dataå®Œæ•´æ€§
        validation_results['master_data_complete'] = self._validate_master_data_complete()
        
        # æª¢æŸ¥é—œè¯æ€§
        if 'item_master' in self.master_data and 'item_inventory' in self.master_data:
            validation_results['item_consistency'] = self._validate_item_consistency()
        
        # ğŸ†• æ–°å¢ï¼šæª¢æŸ¥é€²è²¨è³‡æ–™ä¸€è‡´æ€§
        if 'historical_receiving' in self.transaction_data:
            validation_results['receiving_data_valid'] = self._validate_receiving_data()
        
        # æª¢æŸ¥åƒæ•¸åˆç†æ€§
        if 'system_parameters' in self.master_data:
            validation_results['parameters_reasonable'] = self._validate_parameters()
        
        # ğŸ†• æ–°å¢ï¼šæª¢æŸ¥é€²è²¨èˆ‡å‡ºè²¨è³‡æ–™çš„æ™‚é–“é‡ç–Š
        if ('historical_orders' in self.transaction_data and 
            'historical_receiving' in self.transaction_data):
            validation_results['data_time_overlap'] = self._validate_data_time_overlap()
        
        self.validation_results = validation_results
        
        # è¼¸å‡ºé©—è­‰çµæœ
        for check_name, result in validation_results.items():
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            self.logger.info(f"{check_name}: {status}")
            
        return validation_results
    
    def _validate_receiving_data(self) -> bool:
        """ğŸ†• é©—è­‰é€²è²¨è³‡æ–™å®Œæ•´æ€§"""
        try:
            receiving_df = self.transaction_data['historical_receiving']
            
            if len(receiving_df) == 0:
                self.logger.warning("é€²è²¨è³‡æ–™ç‚ºç©º")
                return False
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_columns = ['DATE', 'FRCD', 'PARTNO', 'QTY']
            missing_columns = [col for col in required_columns if col not in receiving_df.columns]
            
            if missing_columns:
                self.logger.error(f"é€²è²¨è³‡æ–™ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                return False
            
            # æª¢æŸ¥è³‡æ–™å“è³ª
            null_counts = receiving_df[required_columns].isnull().sum()
            total_nulls = null_counts.sum()
            
            if total_nulls > 0:
                self.logger.warning(f"é€²è²¨è³‡æ–™æœ‰ {total_nulls} å€‹ç©ºå€¼: {dict(null_counts)}")
            
            # æª¢æŸ¥é€²è²¨é›¶ä»¶æ˜¯å¦åœ¨item_masterä¸­
            if 'item_master' in self.master_data:
                item_master = self.master_data['item_master']
                valid_items = set(zip(item_master['frcd'], item_master['partno']))
                receiving_items = set(zip(receiving_df['FRCD'], receiving_df['PARTNO']))
                
                invalid_items = receiving_items - valid_items
                if invalid_items:
                    self.logger.warning(f"é€²è²¨è³‡æ–™ä¸­æœ‰ {len(invalid_items)} å€‹é›¶ä»¶ä¸åœ¨item_masterä¸­")
                    # åªè­¦å‘Šï¼Œä¸è¦–ç‚ºå¤±æ•—
            
            return True
            
        except Exception as e:
            self.logger.error(f"é€²è²¨è³‡æ–™é©—è­‰å¤±æ•—: {str(e)}")
            return False
    
    def _validate_data_time_overlap(self) -> bool:
        """ğŸ†• æª¢æŸ¥é€²è²¨èˆ‡å‡ºè²¨è³‡æ–™çš„æ™‚é–“é‡ç–Š"""
        try:
            orders_df = self.transaction_data['historical_orders']
            receiving_df = self.transaction_data['historical_receiving']
            
            # å–å¾—å„è‡ªçš„æ—¥æœŸç¯„åœ
            orders_dates = pd.to_datetime(orders_df['DATE'] if 'DATE' in orders_df.columns 
                                        else orders_df.iloc[:, 0])  # å‡è¨­ç¬¬ä¸€æ¬„æ˜¯æ—¥æœŸ
            receiving_dates = pd.to_datetime(receiving_df['DATE'])
            
            orders_range = (orders_dates.min(), orders_dates.max())
            receiving_range = (receiving_dates.min(), receiving_dates.max())
            
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡ç–Š
            overlap_start = max(orders_range[0], receiving_range[0])
            overlap_end = min(orders_range[1], receiving_range[1])
            
            has_overlap = overlap_start <= overlap_end
            
            if has_overlap:
                overlap_days = (overlap_end - overlap_start).days + 1
                self.logger.info(f"ğŸ“Š é€²è²¨èˆ‡å‡ºè²¨è³‡æ–™é‡ç–Š {overlap_days} å¤© ({overlap_start.strftime('%Y-%m-%d')} - {overlap_end.strftime('%Y-%m-%d')})")
            else:
                self.logger.warning("âš ï¸ é€²è²¨èˆ‡å‡ºè²¨è³‡æ–™æ²’æœ‰æ™‚é–“é‡ç–Š")
            
            return has_overlap
            
        except Exception as e:
            self.logger.error(f"æ™‚é–“é‡ç–Šæª¢æŸ¥å¤±æ•—: {str(e)}")
            return False
    
    def _validate_master_data_complete(self) -> bool:
        """æª¢æŸ¥Master Dataæ˜¯å¦å®Œæ•´"""
        required_files = ['system_parameters', 'item_master', 'workstation_capacity']
        return all(data_name in self.master_data for data_name in required_files)
    
    def _validate_item_consistency(self) -> bool:
        """æª¢æŸ¥é›¶ä»¶è³‡æ–™ä¸€è‡´æ€§"""
        try:
            item_master = self.master_data['item_master']
            item_inventory = self.master_data['item_inventory']
            
            # æª¢æŸ¥é›¶ä»¶ä»£ç¢¼æ˜¯å¦ä¸€è‡´
            master_items = set(zip(item_master['frcd'], item_master['partno']))
            inventory_items = set(zip(item_inventory['frcd'], item_inventory['partno']))
            
            missing_in_inventory = master_items - inventory_items
            missing_in_master = inventory_items - master_items
            
            if missing_in_inventory:
                self.logger.warning(f"åº«å­˜ä¸­ç¼ºå°‘çš„é›¶ä»¶: {len(missing_in_inventory)} å€‹")
            if missing_in_master:
                self.logger.warning(f"ä¸»æª”ä¸­ç¼ºå°‘çš„é›¶ä»¶: {len(missing_in_master)} å€‹")
                
            return len(missing_in_inventory) == 0 and len(missing_in_master) == 0
            
        except Exception as e:
            self.logger.error(f"é›¶ä»¶ä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—: {str(e)}")
            return False
    
    def _validate_parameters(self) -> bool:
        """ğŸ”§ ä¿®æ”¹ï¼šæª¢æŸ¥ç³»çµ±åƒæ•¸åˆç†æ€§ï¼ˆåŒ…å«æ–°åƒæ•¸ï¼‰"""
        try:
            params = self.master_data['system_parameters']
            
            # åŸºæœ¬å¿…è¦åƒæ•¸
            required_params = [
                'daily_work_hours',
                'picking_base_time_repack',
                'picking_base_time_no_repack'
            ]
            
            # ğŸ†• æ–°å¢çš„å¿…è¦åƒæ•¸
            new_required_params = [
                'receiving_completion_days',
                'shift_start_time',
                'shift_end_time'
            ]
            
            all_required = required_params + new_required_params
            param_names = params['parameter_name'].tolist()
            missing_params = [p for p in all_required if p not in param_names]
            
            if missing_params:
                self.logger.error(f"ç¼ºå°‘å¿…è¦åƒæ•¸: {missing_params}")
                return False
            
            # ğŸ†• æª¢æŸ¥æ–°åƒæ•¸çš„åˆç†æ€§
            validation_checks = {
                'receiving_completion_days': lambda x: 1 <= int(x) <= 7,  # 1-7å¤©
                'max_overtime_hours': lambda x: 0.5 <= float(x) <= 6.0,   # 0.5-6å°æ™‚
            }
            
            for param_name, validator in validation_checks.items():
                param_row = params[params['parameter_name'] == param_name]
                if len(param_row) > 0:
                    try:
                        value = param_row.iloc[0]['parameter_value']
                        if not validator(value):
                            self.logger.warning(f"åƒæ•¸å€¼ä¸åˆç†: {param_name} = {value}")
                    except Exception as e:
                        self.logger.warning(f"åƒæ•¸é©—è­‰å¤±æ•—: {param_name} - {str(e)}")
                        
            return True
            
        except Exception as e:
            self.logger.error(f"åƒæ•¸æª¢æŸ¥å¤±æ•—: {str(e)}")
            return False
    
    def get_parameter_value(self, parameter_name: str, default_value=None):
        """å–å¾—ç³»çµ±åƒæ•¸å€¼"""
        if 'system_parameters' not in self.master_data:
            return default_value
            
        params = self.master_data['system_parameters']
        param_row = params[params['parameter_name'] == parameter_name]
        
        if len(param_row) == 0:
            self.logger.warning(f"åƒæ•¸ {parameter_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­å€¼: {default_value}")
            return default_value
            
        value = param_row.iloc[0]['parameter_value']
        data_type = param_row.iloc[0]['data_type']
        
        # æ ¹æ“šè³‡æ–™å‹æ…‹è½‰æ›
        try:
            if data_type == 'integer':
                return int(value)
            elif data_type == 'float':
                return float(value)
            else:
                return str(value)
        except ValueError:
            self.logger.warning(f"åƒæ•¸ {parameter_name} å€¼è½‰æ›å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼: {default_value}")
            return default_value

    def filter_valid_items(self, transaction_df: pd.DataFrame) -> pd.DataFrame:
        """éæ¿¾æœ‰æ•ˆé›¶ä»¶ï¼ˆåªä¿ç•™item_masterä¸­å­˜åœ¨çš„é›¶ä»¶ï¼‰"""
        if 'item_master' not in self.master_data:
            self.logger.warning("item_masteræœªè¼‰å…¥ï¼Œç„¡æ³•éæ¿¾é›¶ä»¶")
            return transaction_df
        
        # å–å¾—æœ‰æ•ˆé›¶ä»¶æ¸…å–®
        item_master = self.master_data['item_master']
        valid_items = set(zip(item_master['frcd'], item_master['partno']))
        
        # éæ¿¾å‰çš„è³‡æ–™é‡
        original_count = len(transaction_df)
        
        # æª¢æŸ¥transaction_dfæ˜¯å¦æœ‰frcdå’Œpartnoæ¬„ä½
        if 'FRCD' in transaction_df.columns and 'PARTNO' in transaction_df.columns:
            # å»ºç«‹éæ¿¾æ¢ä»¶ï¼ˆæ³¨æ„æ¬„ä½åç¨±å¤§å°å¯«ï¼‰
            transaction_items = list(zip(transaction_df['FRCD'], transaction_df['PARTNO']))
            valid_mask = [item in valid_items for item in transaction_items]
            
            # æ‡‰ç”¨éæ¿¾
            filtered_df = transaction_df[valid_mask].copy()
            
            filtered_count = len(filtered_df)
            removed_count = original_count - filtered_count
            
            if removed_count > 0:
                self.logger.info(f"é›¶ä»¶éæ¿¾: åŸå§‹ {original_count} ç­†ï¼Œç§»é™¤ {removed_count} ç­†ç„¡æ•ˆé›¶ä»¶ï¼Œä¿ç•™ {filtered_count} ç­†")
            else:
                self.logger.info(f"é›¶ä»¶éæ¿¾: æ‰€æœ‰ {original_count} ç­†è³‡æ–™éƒ½æ˜¯æœ‰æ•ˆé›¶ä»¶")
            
            return filtered_df
        elif 'frcd' in transaction_df.columns and 'partno' in transaction_df.columns:
            # å°å¯«æ¬„ä½åç¨±ç‰ˆæœ¬
            transaction_items = list(zip(transaction_df['frcd'], transaction_df['partno']))
            valid_mask = [item in valid_items for item in transaction_items]
            
            filtered_df = transaction_df[valid_mask].copy()
            
            filtered_count = len(filtered_df)
            removed_count = original_count - filtered_count
            
            if removed_count > 0:
                self.logger.info(f"é›¶ä»¶éæ¿¾: åŸå§‹ {original_count} ç­†ï¼Œç§»é™¤ {removed_count} ç­†ç„¡æ•ˆé›¶ä»¶ï¼Œä¿ç•™ {filtered_count} ç­†")
            
            return filtered_df
        else:
            self.logger.warning("äº¤æ˜“è³‡æ–™ç¼ºå°‘frcd/FRCDæˆ–partno/PARTNOæ¬„ä½ï¼Œç„¡æ³•éæ¿¾")
            return transaction_df

    def get_valid_items_summary(self) -> Dict:
        """å–å¾—æœ‰æ•ˆé›¶ä»¶æ‘˜è¦çµ±è¨ˆ"""
        if 'item_master' not in self.master_data:
            return {}
        
        item_master = self.master_data['item_master']
        
        # æŒ‰æ¨“å±¤çµ±è¨ˆ
        floor_stats = item_master['floor'].value_counts().to_dict()
        
        # æŒ‰é›¶ä»¶å‰ç¢¼çµ±è¨ˆ
        frcd_stats = item_master['frcd'].value_counts().head(10).to_dict()
        
        return {
            'total_valid_items': len(item_master),
            'items_by_floor': floor_stats,
            'top_10_frcd': frcd_stats,
            'unique_frcd_count': item_master['frcd'].nunique(),
            'repack_ratio': (item_master['repack'] == 'Y').mean() if 'repack' in item_master.columns else 0
        }
    
    def get_receiving_data_summary(self) -> Dict:
        """ğŸ†• å–å¾—é€²è²¨è³‡æ–™æ‘˜è¦"""
        if not self.receiving_data_available:
            return {'available': False, 'message': 'ç„¡é€²è²¨è³‡æ–™'}
        
        receiving_df = self.transaction_data['historical_receiving']
        
        summary = {
            'available': True,
            'total_records': len(receiving_df),
            'date_range': self.receiving_date_range,
            'unique_items': len(receiving_df[['FRCD', 'PARTNO']].drop_duplicates()),
            'total_quantity': receiving_df['QTY'].sum(),
            'avg_daily_records': 0,
            'top_item_types': {},
            'quantity_distribution': {}
        }
        
        # è¨ˆç®—å¹³å‡æ¯æ—¥ç­†æ•¸
        if self.receiving_date_range:
            summary['avg_daily_records'] = round(
                summary['total_records'] / self.receiving_date_range['total_days'], 1
            )
        
        # çµ±è¨ˆæœ€å¸¸è¦‹çš„é›¶ä»¶é¡å‹
        summary['top_item_types'] = receiving_df['FRCD'].value_counts().head(5).to_dict()
        
        # æ•¸é‡åˆ†å¸ƒ
        summary['quantity_distribution'] = {
            'min': int(receiving_df['QTY'].min()),
            'max': int(receiving_df['QTY'].max()),
            'mean': round(receiving_df['QTY'].mean(), 1),
            'median': int(receiving_df['QTY'].median())
        }
        
        return summary
    
    def export_data_summary(self) -> Dict:
        """ğŸ†• åŒ¯å‡ºå®Œæ•´çš„è³‡æ–™æ‘˜è¦"""
        summary = {
            'master_data': {},
            'transaction_data': {},
            'validation_results': self.validation_results,
            'export_time': datetime.now().isoformat()
        }
        
        # Master Dataæ‘˜è¦
        for data_name, df in self.master_data.items():
            summary['master_data'][data_name] = {
                'record_count': len(df),
                'columns': list(df.columns),
                'file_size_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
            }
        
        # Transaction Dataæ‘˜è¦
        for data_name, df in self.transaction_data.items():
            summary['transaction_data'][data_name] = {
                'record_count': len(df),
                'columns': list(df.columns),
                'file_size_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2)
            }
            
            # ç‰¹æ®Šè™•ç†ï¼šæ—¥æœŸç¯„åœ
            if 'DATE' in df.columns:
                date_col = pd.to_datetime(df['DATE'])
                summary['transaction_data'][data_name]['date_range'] = {
                    'start': date_col.min().strftime('%Y-%m-%d'),
                    'end': date_col.max().strftime('%Y-%m-%d'),
                    'days': (date_col.max() - date_col.min()).days + 1
                }
        
        # é€²è²¨è³‡æ–™ç‰¹æ®Šæ‘˜è¦
        if self.receiving_data_available:
            summary['receiving_summary'] = self.get_receiving_data_summary()
        
        return summary
    
    @staticmethod
    def is_workday(target_date):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºå·¥ä½œæ—¥ï¼ˆé€±ä¸€åˆ°é€±äº”ï¼‰
        
        Args:
            target_date: datetime.date æˆ– datetime.datetime ç‰©ä»¶
        
        Returns:
            bool: True ç‚ºå·¥ä½œæ—¥ï¼ŒFalse ç‚ºé€±æœ«
        """
        if hasattr(target_date, 'weekday'):
            weekday = target_date.weekday()  # 0=é€±ä¸€, 6=é€±æ—¥
            return weekday < 5  # 0-4 ç‚ºé€±ä¸€åˆ°é€±äº”
        return False
    
    def _clean_master_data(self, df: pd.DataFrame, data_name: str) -> pd.DataFrame:
        """ğŸ†• æ–°å¢ï¼šæ¸…ç†master data"""
        try:
            # ç§»é™¤æ‰€æœ‰æ¬„ä½çš„å‰å¾Œç©ºæ ¼
            for col in df.columns:
                if df[col].dtype == 'object':  # åªè™•ç†æ–‡å­—æ¬„ä½
                    df[col] = df[col].astype(str).str.strip()
            
            # ç‰¹åˆ¥è™•ç† route_schedule_master
            if data_name == 'route_schedule_master':
                # æ¸…ç†å¯èƒ½çš„ç©ºæ ¼å•é¡Œ
                if 'PARTCUSTID' in df.columns:
                    df['PARTCUSTID'] = df['PARTCUSTID'].str.strip()
                
                if 'ROUTECD' in df.columns:
                    df['ROUTECD'] = df['ROUTECD'].str.strip()
                
                # ç¢ºä¿æ™‚é–“æ¬„ä½æ˜¯æ•¸å­—æ ¼å¼
                for time_col in ['ORDERENDTIME', 'DELIVERTM']:
                    if time_col in df.columns:
                        # ç§»é™¤ç©ºæ ¼ä¸¦è½‰ç‚ºå­—ä¸²
                        df[time_col] = df[time_col].astype(str).str.strip()
                        # ç§»é™¤ 'nan' å­—ä¸²
                        df[time_col] = df[time_col].replace('nan', '')
                        
                        self.logger.debug(f"æ¸…ç† {time_col} æ¬„ä½å®Œæˆ")
                
                self.logger.info(f"route_schedule_master æ•¸æ“šæ¸…ç†å®Œæˆ")
            
            return df
            
        except Exception as e:
            self.logger.warning(f"æ¸…ç† {data_name} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return df